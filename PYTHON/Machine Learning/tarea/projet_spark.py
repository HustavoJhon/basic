# -*- coding: utf-8 -*-
"""Projet_Spark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E4XUw0MJe6r2_hQdgCkOG8hvx7EXeDrX
"""

#TRABAJO FINAL BIG DATA Y MACHINE LEARNING
#ISAIAS GUZMÁN JORDÁN
#SENATI 5TO SEMESTRE
#ING DE SOFTWARE CON IA

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

!wget -q https://apache.osuosl.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz
!tar xf spark-3.2.0-bin-hadoop3.2.tgz

!pip install pyspark==3.2.0

# Importar las bibliotecas necesarias
from pyspark.sql import SparkSession

# Crear una sesión de Spark
spark = SparkSession.builder.appName("Analisis").getOrCreate()

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/My Drive/Colab Notebooks
# Leer el archivo CSV con el esquema especificado
# Leer el archivo CSV con el esquema especificado
df = spark.read.format("csv").option("header", "true").load("/content/drive/My Drive/Colab Notebooks/main_product_sales_weather_view.csv")

# Mostrar el dataframe
df.show()

import matplotlib.pyplot as plt
import numpy as np
from pyspark.sql.functions import col
# Calcular el histograma
hist, bin_edges = np.histogram(df.select(col("daily_sales_count").cast("float")).rdd.flatMap(lambda x: x).collect(), bins=10)
# Graficar el histograma
plt.hist(df.select(col("daily_sales_count")).rdd.flatMap(lambda x: x).collect(), bins=10)
plt.xlabel("Valor")
plt.xticks(bin_edges)
plt.ylabel("Frecuencia")
plt.show()

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import GBTRegressor
from pyspark.ml.evaluation import RegressionEvaluator

selected_cols = ["sales_date", "weekday_name", "daily_sales_count", "daily_sales_value", "dominant_weather_type"]
df_selected = df.select(selected_cols)

from pyspark.sql.functions import dayofweek, month

df_selected = df_selected.withColumn("weekday", dayofweek(df_selected["sales_date"]))
df_selected = df_selected.withColumn("month", month(df_selected["sales_date"]))

df_selected = df_selected.drop("sales_date")
df_selected = df_selected.drop("weekday_name")

from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler

# Codificar las variables categóricas
categorical_cols = ["dominant_weather_type"]
for col in categorical_cols:
    string_indexer = StringIndexer(inputCol=col, outputCol=col+"_index")
    encoder = OneHotEncoder(inputCols=[string_indexer.getOutputCol()], outputCols=[col+"_encoded"])
    df_selected = string_indexer.fit(df_selected).transform(df_selected)
    df_selected = encoder.fit(df_selected).transform(df_selected)

from pyspark.sql.functions import col

# Convertir las columnas "daily_sales_count" y "daily_sales_value" a tipo numérico
df_selected = df_selected.withColumn("daily_sales_count", col("daily_sales_count").cast("float"))
df_selected = df_selected.withColumn("daily_sales_value", col("daily_sales_value").cast("float"))

# Escalar las variables numéricas
numeric_cols = ["daily_sales_count", "daily_sales_value"]
assembler = VectorAssembler(inputCols=[col+"_encoded" for col in categorical_cols] + numeric_cols, outputCol="features")
df_selected = assembler.transform(df_selected)

scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
scaler_model = scaler.fit(df_selected)
df_scaled = scaler_model.transform(df_selected)

df_final = df_scaled.select("scaled_features", "daily_sales_count")

(train_df, test_df) = df_final.randomSplit([0.7, 0.3])

from pyspark.ml.regression import GBTRegressor

gbt = GBTRegressor(featuresCol="scaled_features", labelCol="daily_sales_count", maxDepth=5, maxBins=32, maxIter=20, seed=42)
gbt_model = gbt.fit(train_df)

# Realizar predicciones con los datos de prueba
predictions = gbt_model.transform(test_df)

# Mostrar las predicciones
predictions.show()
# -*- coding: utf-8 -*-
"""Projet_Spark.ipynb

Automatically generated by Colaboratory.
        ;     /        ,--.     
       ["]   ["]  ,<  |__**|    
      /[_]\  [~]\/    |//  |    
       ] [   OOO      /o|__|   
"""


# !apt-get install openjdk-8-jdk-headless -qq > /dev/null

# !wget -q https://apache.osuosl.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz
# !tar xf spark-3.2.0-bin-hadoop3.2.tgz

# !pip install pyspark==3.2.0

# Importar las bibliotecas necesarias
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Analisis").getOrCreate()

from google.colab import drive
drive.mount('/content/drive')
df = spark.read.format("csv").option("header", "true").load("/content/drive/My Drive/Colab Notebooks/main_product_sales_weather_view.csv")

df.show()

import matplotlib.pyplot as plt
import numpy as np
from pyspark.sql.functions import col
hist, bin_edges = np.histogram(df.select(col("daily_sales_count").cast("float")).rdd.flatMap(lambda x: x).collect(), bins=10)
plt.hist(df.select(col("daily_sales_count")).rdd.flatMap(lambda x: x).collect(), bins=10)
plt.xlabel("Valor")
plt.xticks(bin_edges)
plt.ylabel("Frecuencia")
plt.show()

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import GBTRegressor
from pyspark.ml.evaluation import RegressionEvaluator

selected_cols = ["sales_date", "weekday_name", "daily_sales_count", "daily_sales_value", "dominant_weather_type"]
df_selected = df.select(selected_cols)

from pyspark.sql.functions import dayofweek, month

df_selected = df_selected.withColumn("weekday", dayofweek(df_selected["sales_date"]))
df_selected = df_selected.withColumn("month", month(df_selected["sales_date"]))

df_selected = df_selected.drop("sales_date")
df_selected = df_selected.drop("weekday_name")

from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler

# Codificar las variables categóricas
categorical_cols = ["dominant_weather_type"]
for col in categorical_cols:
    string_indexer = StringIndexer(inputCol=col, outputCol=col+"_index")
    encoder = OneHotEncoder(inputCols=[string_indexer.getOutputCol()], outputCols=[col+"_encoded"])
    df_selected = string_indexer.fit(df_selected).transform(df_selected)
    df_selected = encoder.fit(df_selected).transform(df_selected)

from pyspark.sql.functions import col

# Convertir las columnas "daily_sales_count" y "daily_sales_value" a tipo numérico
df_selected = df_selected.withColumn("daily_sales_count", col("daily_sales_count").cast("float"))
df_selected = df_selected.withColumn("daily_sales_value", col("daily_sales_value").cast("float"))

# Escalar las variables numéricas
numeric_cols = ["daily_sales_count", "daily_sales_value"]
assembler = VectorAssembler(inputCols=[col+"_encoded" for col in categorical_cols] + numeric_cols, outputCol="features")
df_selected = assembler.transform(df_selected)

scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
scaler_model = scaler.fit(df_selected)
df_scaled = scaler_model.transform(df_selected)

df_final = df_scaled.select("scaled_features", "daily_sales_count")

(train_df, test_df) = df_final.randomSplit([0.7, 0.3])

from pyspark.ml.regression import GBTRegressor

gbt = GBTRegressor(featuresCol="scaled_features", labelCol="daily_sales_count", maxDepth=5, maxBins=32, maxIter=20, seed=42)
gbt_model = gbt.fit(train_df)

predictions = gbt_model.transform(test_df)

predictions.show()
